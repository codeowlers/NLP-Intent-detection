{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from algorithms import *\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract development and evaulation\n",
    "df = pd.read_csv(\"dsl_data/development.csv\")\n",
    "df_eval = pd.read_csv(\"dsl_data/evaluation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['intent'] = df['action']+ df['object']\n",
    "y = df['intent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Id','Self-reported fluency level ', 'First Language spoken', 'Current language used for work/school','action','object','intent']\n",
    "df.drop(columns=cols,inplace=True)\n",
    "df_eval.drop(columns=cols[:4],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder(df, 'gender')\n",
    "label_encoder(df_eval, 'gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder(df, 'ageRange')\n",
    "label_encoder(df_eval, 'ageRange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_feature_extraction(df)\n",
    "audio_feature_extraction(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "librosa.get_duration(y=df['data'][108], sr=df['rate'][108])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_audio(row):\n",
    "    data = row['data']\n",
    "    trimmed_data, index = librosa.effects.trim(data, top_db=20, frame_length=2048, hop_length=512)\n",
    "    return trimmed_data\n",
    "\n",
    "df['data'] = df.apply(trim_audio, axis=1)\n",
    "df_eval['data'] = df_eval.apply(trim_audio, axis=1)\n",
    "\n",
    "# df['data'] = df['data'].apply(lambda x: np.hstack([x[part[0]:part[-1]] for part in librosa.effects.split(x, top_db=30)]))\n",
    "# df_eval['data'] = df_eval['data'].apply(lambda x: np.hstack([x[part[0]:part[-1]] for part in librosa.effects.split(x, top_db=30)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.253877551020408"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "librosa.get_duration(y=df['data'][108], sr=df['rate'][108])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spectrogram(df):\n",
    "    spectrograms = []\n",
    "    for data,rate in zip(df['data'], df['rate']):\n",
    "        \n",
    "        # Compute the spectrogram\n",
    "        spectrogram = librosa.feature.melspectrogram(y=data, sr=rate)\n",
    "        \n",
    "        # Split the spectrogram into 20 chunks\n",
    "        n_chunks = 20\n",
    "        chunks = np.array_split(spectrogram, n_chunks)\n",
    "        \n",
    "        # Append the chunks to the list of spectrograms\n",
    "        spectrograms.append(chunks)\n",
    "    \n",
    "    # Add the spectrogram chunks to the dataframe as new columns\n",
    "    for i in range(n_chunks):\n",
    "        df[\"mean_spectrogram_\" + str(i)] = [np.mean(chunk[i]) for chunk in spectrograms]\n",
    "        df[\"std_spectrogram_\" + str(i)] = [np.std(chunk[i]) for chunk in spectrograms]\n",
    "    # return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_spectrogram(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_spectrogram(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_feature(df)\n",
    "mfcc_feature(df_eval)\n",
    "time_domain_2D(df,'mfcc')\n",
    "time_domain_2D(df_eval,'mfcc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_column_spread(df,'mfcc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mfcc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_mfcc(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_mfcc(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_feature(df)\n",
    "rmse_feature(df_eval)\n",
    "\n",
    "time_domain_1D(df,'rmse')\n",
    "time_domain_1D(df_eval,'rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sro_feature(df)\n",
    "sro_feature(df_eval)\n",
    "time_domain_1D(df,'spectral_rolloff')\n",
    "time_domain_1D(df_eval,'spectral_rolloff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_flatness(df)\n",
    "spectral_flatness(df_eval)\n",
    "time_domain_1D(df,'sf')\n",
    "time_domain_1D(df_eval,'sf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zcr_feature(df)\n",
    "zcr_feature(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chroma(dataframe):\n",
    "    chroma_array = []\n",
    "    for data, rate in zip(dataframe['data'], dataframe['rate']):\n",
    "        # Load the audio file\n",
    "        chroma = librosa.feature.chroma_stft(y=data, sr=rate, n_chroma=7)       \n",
    "        # Split the spectrogram into 20 chunks\n",
    "        n_chunks = 20\n",
    "        chunks = np.array_split(chroma, n_chunks)\n",
    "        \n",
    "        # Append the chunks to the list of spectrograms\n",
    "        chroma_array.append(chunks)\n",
    "    \n",
    "    # Add the spectrogram chunks to the dataframe as new columns\n",
    "    for i in range(n_chunks):\n",
    "        dataframe[\"mean_chroma_\" + str(i)] = [np.mean(chunk[i]) for chunk in chroma_array]\n",
    "        dataframe[\"std_chroma_\" + str(i)] = [np.std(chunk[i]) for chunk in chroma_array]\n",
    "    return dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_chroma(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_chroma(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tonnetz(dataframe):\n",
    "    tonnetz_array = []\n",
    "    for data, rate in zip(dataframe['data'], dataframe['rate']):\n",
    "        # Load the audio file\n",
    "        tonnetz = librosa.feature.tonnetz(y=data, sr=rate)\n",
    "        # Split the spectrogram into 20 chunks\n",
    "        n_chunks = 20\n",
    "        chunks = np.array_split(tonnetz, n_chunks)\n",
    "        \n",
    "        # Append the chunks to the list of spectrograms\n",
    "        tonnetz_array.append(chunks)\n",
    "    \n",
    "    # Add the spectrogram chunks to the dataframe as new columns\n",
    "    for i in range(n_chunks):\n",
    "        dataframe[\"mean_tonnetz_\" + str(i)] = [np.mean(chunk[i]) for chunk in tonnetz_array]\n",
    "        dataframe[\"std_tonnetz_\" + str(i)] = [np.std(chunk[i]) for chunk in tonnetz_array]\n",
    "    return dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_tonnetz(df)\n",
    "extract_tonnetz(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spectral_contrast(dataframe):\n",
    "    spectral_contrast_array = []\n",
    "    for data, rate in zip(dataframe['data'], dataframe['rate']):\n",
    "        # Load the audio file\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=data, sr=rate)\n",
    "        # Split the spectrogram into 20 chunks\n",
    "        n_chunks = 20\n",
    "        chunks = np.array_split(spectral_contrast, n_chunks)\n",
    "        \n",
    "        # Append the chunks to the list of spectrograms\n",
    "        spectral_contrast_array.append(chunks)\n",
    "    \n",
    "    # Add the spectrogram chunks to the dataframe as new columns\n",
    "    for i in range(n_chunks):\n",
    "        dataframe[\"mean_tspectral_contrast_\" + str(i)] = [np.mean(chunk[i]) for chunk in spectral_contrast_array]\n",
    "        dataframe[\"std_spectral_contrast_\" + str(i)] = [np.std(chunk[i]) for chunk in spectral_contrast_array]\n",
    "    return dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_spectral_contrast(df)\n",
    "extract_spectral_contrast(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.drop(columns=['speakerId','path','data', 'rate'],inplace= True)\n",
    "df.drop(columns=['speakerId','path','data', 'rate'],inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "X_eval = df_eval.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(columns=['rmse','sf','spectral_rolloff'], inplace=True)\n",
    "X_eval.drop(columns=['rmse','sf','spectral_rolloff'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.to_csv('x.csv',index=False)\n",
    "        \n",
    "# X_eval.to_csv('x_eval.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.dropna(axis=1, how='all')\n",
    "X_eval = X_eval.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_eval = pd.read_csv('x_eval.csv')\n",
    "# X = pd.read_csv('x.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip to here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def calculate_class_counts(X, y):\n",
    "#     class_counts = {}\n",
    "#     for target in set(y):\n",
    "#         class_counts[target] = y.tolist().count(target)\n",
    "#     return class_counts\n",
    "\n",
    "# print(calculate_class_counts(X_new, y_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X =  normalize_dataframe(X)\n",
    "# X_eval = normalize_dataframe(X_eval)\n",
    "def standardize(x_train, x_test):\n",
    "    mean = np.mean(x_train, axis=0)\n",
    "    std = np.std(x_train, axis=0)\n",
    "    x_train_standardized = (x_train - mean) / std\n",
    "    x_test_standardized = (x_test - mean) / std\n",
    "    return x_train_standardized, x_test_standardized\n",
    "X, X_eval = standardize(X, X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = normalize_dataframe(X)\n",
    "X_eval=normalize_dataframe(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {'C': [0.1, 1, 5, 10, 100, 1000],\n",
    "              'gamma': [0.001, 0.01, 0.1, 1, 'scale', 'auto'],\n",
    "              'kernel': ['rbf']}\n",
    "\n",
    "# Create an SVM classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Perform the grid search using 5-fold cross validation\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def svm_model2(X_train, y_train, X_test):\n",
    "    clf = SVC(gamma=1,C=1000,random_state=42)\n",
    "    # train the model on the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "    # predict the target values for the test data\n",
    "    # returning the y_predict\n",
    "    return clf.predict(X_test)\n",
    "y_pred = svm_model2(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_calculator(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def svm_model2(X_train, y_train, X_test):\n",
    "    clf = SVC(gamma=1,C=1000,random_state=42)\n",
    "    # train the model on the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "    # predict the target values for the test data\n",
    "    # returning the y_predict\n",
    "    return clf.predict(X_test)\n",
    "y_pred = svm_model2(X, y, X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_decoded = label_encoder.inverse_transform(y_pred)\n",
    "pd.Series(y_decoded).to_csv('predictions.csv', index='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def select_top_n_features(X_train, X_test, threshold):\n",
    "    pca = PCA()\n",
    "    \n",
    "    # Fit PCA on the training data\n",
    "    pca.fit(X_train)\n",
    "    \n",
    "    # Get the variance ratio for each principal component\n",
    "    variance_ratio = pca.explained_variance_ratio_\n",
    "    n_components = len(variance_ratio)\n",
    "    \n",
    "    # Find the number of components that explain at least the threshold of variance\n",
    "    explained_variance = 0\n",
    "    for i, ratio in enumerate(variance_ratio):\n",
    "        explained_variance += ratio\n",
    "        if explained_variance >= threshold:\n",
    "            n_components = i + 1\n",
    "            break\n",
    "    print(f\"Number of components that explain at least {threshold} of variance : {n_components}\")\n",
    "    # Initialize the PCA object with the number of components\n",
    "    pca = PCA(n_components=n_components)\n",
    "    \n",
    "    # Fit PCA on the training data\n",
    "    pca.fit(X_train)\n",
    "    # Transform the training and test data to the first n principal components\n",
    "    X_train_top_n_features = pca.transform(X_train)\n",
    "    X_test_top_n_features = pca.transform(X_test)\n",
    "    #Return the top features and the variance ratio\n",
    "    return X_train_top_n_features, X_test_top_n_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a list of all possible values for each attribute\n",
    "n_estimators = np.arange(10, 100, 10)\n",
    "max_depth = [None, 5, 10, 15, 20]\n",
    "min_samples_split = np.arange(2, 20, 2)\n",
    "min_samples_leaf = np.arange(1, 20, 2)\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "\n",
    "# Create a list of all possible combinations of the attributes\n",
    "combinations = [(a,b,c,d,e) for a in n_estimators for b in max_depth for c in min_samples_split for d in min_samples_leaf for e in max_features]\n",
    "\n",
    "# Select the first four combinations\n",
    "selected_combinations = combinations[:4]\n",
    "\n",
    "# Print the selected combinations\n",
    "print(selected_combinations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "accuracy = []\n",
    "\n",
    "# Generate some sample data\n",
    "\n",
    "for comb in combinations:\n",
    "\n",
    "    # Initialize the Random Forest classifier\n",
    "    rf = RandomForestClassifier(n_estimators = comb[0],max_depth=comb[1], min_samples_split = comb[2], min_samples_leaf=  comb[3], max_features= comb[4])\n",
    "\n",
    "    # Fit the classifier to the data\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on new data\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    # Print the accuracy of the model\n",
    "    print(comb)\n",
    "    print(\"Accuracy:\", rf.score(X_test, y_test))\n",
    "    accuracy.append((rf.score(X_test, y_test),comb))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_decoded = label_encoder.inverse_transform(y_pred)\n",
    "pd.Series(y_decoded).to_csv('predictions.csv', index='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec 15 2022, 17:11:09) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09ee11da7dc62f10eaf9df9c4559184832adcd88d836286cbf9804f9d006cf55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
