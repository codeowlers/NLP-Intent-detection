{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from algorithms import *\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract development and evaulation\n",
    "df = pd.read_csv(\"dsl_data/development.csv\")\n",
    "df_eval = pd.read_csv(\"dsl_data/evaluation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = (df['Current language used for work/school'] == np.unique(df_eval['Current language used for work/school'].values)[0]) & (df['First Language spoken'] == np.unique(df_eval['First Language spoken'].values)[0]) & (df['Self-reported fluency level '] == np.unique(df_eval['Self-reported fluency level '].values)[0])\n",
    "df = df.loc[condition]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['intent'] = df['action']+ df['object']\n",
    "# encoding y\n",
    "dict = {}\n",
    "for i, el in enumerate(df['intent'].unique()):\n",
    "    dict[el] = i\n",
    "\n",
    "df['intent'] = df['intent'].apply(lambda x: dict[x])\n",
    "y = df['intent'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping irrelevant columns\n",
    "df.drop(columns=[\"Self-reported fluency level \",\"First Language spoken\", \"Current language used for work/school\",\"action\",\"object\", \"intent\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping irrelevant columns\n",
    "df_eval.drop(columns=[\"Self-reported fluency level \",\"First Language spoken\", \"Current language used for work/school\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder(df, 'gender')\n",
    "label_encoder(df_eval, 'gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder(df, 'ageRange')\n",
    "label_encoder(df_eval, 'ageRange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_feature_extraction(df)\n",
    "audio_feature_extraction(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def extract_spectrogram(dataframe):\n",
    "    audio_paths = dataframe[\"path\"].values\n",
    "    spectrograms = []\n",
    "    for data in dataframe['data']:\n",
    "        \n",
    "        # Compute the spectrogram\n",
    "        spectrogram = np.abs(librosa.stft(data))\n",
    "        \n",
    "        # Split the spectrogram into 20 chunks\n",
    "        n_chunks = 20\n",
    "        chunks = np.array_split(spectrogram, n_chunks)\n",
    "        \n",
    "        # Append the chunks to the list of spectrograms\n",
    "        spectrograms.append(chunks)\n",
    "    \n",
    "    # Add the spectrogram chunks to the dataframe as new columns\n",
    "    for i in range(n_chunks):\n",
    "        dataframe[\"mean_spectrogram_\" + str(i)] = [np.mean(chunk[i]) for chunk in spectrograms]\n",
    "        dataframe[\"std_spectrogram_\" + str(i)] = [np.std(chunk[i]) for chunk in spectrograms]\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_spectrogram(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_spectrogram(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(dataframe):\n",
    "    mfcc_array = []\n",
    "    for data, rate in zip(dataframe['data'], dataframe['rate']):\n",
    "        # Load the audio file\n",
    "        mfcc = librosa.feature.mfcc(y=data, sr=rate, n_mfcc=30)        \n",
    "        # Split the spectrogram into 20 chunks\n",
    "        n_chunks = 20\n",
    "        chunks = np.array_split(mfcc, n_chunks)\n",
    "        \n",
    "        # Append the chunks to the list of spectrograms\n",
    "        mfcc_array.append(chunks)\n",
    "    \n",
    "    # Add the spectrogram chunks to the dataframe as new columns\n",
    "    for i in range(n_chunks):\n",
    "        dataframe[\"mean_mfcc_\" + str(i)] = [np.mean(chunk[i]) for chunk in mfcc_array]\n",
    "        dataframe[\"std_mfcc_\" + str(i)] = [np.std(chunk[i]) for chunk in mfcc_array]\n",
    "    return dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_mfcc(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_mfcc(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.drop(columns=['Id','ageRange','data','speakerId','path', 'rate'],inplace= True)\n",
    "df.drop(columns=['Id','ageRange','data','speakerId','path', 'rate'],inplace= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "X_eval = df_eval.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zcr_feature(df)\n",
    "zcr_feature(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X.to_csv('x.csv',index=False)\n",
    "\n",
    "\n",
    "X_eval.to_csv('x_eval.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eval = pd.read_csv('x_eval.csv')\n",
    "X = pd.read_csv('x.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X)\n",
    "scaler.fit(X_eval)\n",
    "\n",
    "X = scaler.transform(X)\n",
    "X_eval = scaler.transform(X_eval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf'], 'gamma': [0.01, 0.1, 1, 10]}\n",
    "\n",
    "# Initialize the SVC\n",
    "svc = SVC()\n",
    "\n",
    "# Initialize the GridSearchCV\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "y = np.array([0, 1, 2])\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf'], 'gamma': [0.1, 1, 10]}\n",
    "\n",
    "# Initialize the SVC\n",
    "svc = SVC()\n",
    "\n",
    "# Initialize the KFold\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Initialize the GridSearchCV\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=kf)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_model2(X_train, y_train, X_test):\n",
    "    clf = SVC(C = 100, gamma = 0.01, kernel = 'linear')\n",
    "    # train the model on the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "    # predict the target values for the test data\n",
    "    # returning the y_predict\n",
    "    return clf.predict(X_test)\n",
    "y_pred = svm_model2(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4163113006396588"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_calculator(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_mapping = np.vectorize(lambda x: next(key for key, value in dict.items() if value == x))\n",
    "keys_arr = key_mapping(y_pred)\n",
    "pd.Series(keys_arr).to_csv('predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09ee11da7dc62f10eaf9df9c4559184832adcd88d836286cbf9804f9d006cf55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
