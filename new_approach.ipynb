{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from algorithms import *\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract development and evaulation\n",
    "df = pd.read_csv(\"dsl_data/development.csv\")\n",
    "df_eval = pd.read_csv(\"dsl_data/evaluation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = (df['Current language used for work/school'] == np.unique(df_eval['Current language used for work/school'].values)[0]) & (df['First Language spoken'] == np.unique(df_eval['First Language spoken'].values)[0]) & (df['Self-reported fluency level '] == np.unique(df_eval['Self-reported fluency level '].values)[0])\n",
    "df = df.loc[condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['intent'] = df['action']+ df['object']\n",
    "y = df['intent']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder(df, 'gender')\n",
    "label_encoder(df_eval, 'gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping irrelevant columns\n",
    "# df.drop(columns=[\"Self-reported fluency level \",\"First Language spoken\", \"Current language used for work/school\",\"action\",\"object\", \"intent\"], inplace=True)\n",
    "df.drop(columns=[\"action\",\"object\", \"intent\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #dropping irrelevant columns\n",
    "# df_eval.drop(columns=[\"Self-reported fluency level \",\"First Language spoken\", \"Current language used for work/school\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder(df, 'gender')\n",
    "label_encoder(df_eval, 'gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder(df, 'ageRange')\n",
    "label_encoder(df_eval, 'ageRange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_feature_extraction(df)\n",
    "audio_feature_extraction(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def remove_silence(y, sr, threshold=0.01):\n",
    "    rmse = librosa.feature.rms(y=y, frame_length=2048, hop_length=512)\n",
    "    y_silent = np.array([0 if val <= threshold else val for val in rmse[0]])\n",
    "    y_without_silence = y[y_silent != 0]\n",
    "    sr_without_silence = sr\n",
    "    return y_without_silence, sr_without_silence\n",
    "\n",
    "y, sr = df[\"data\"][0], df[\"rate\"][0]\n",
    "y_without_silence, sr_without_silence = remove_silence(y, sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def extract_spectrogram(dataframe):\n",
    "    audio_paths = dataframe[\"path\"].values\n",
    "    spectrograms = []\n",
    "    for data in dataframe['data']:\n",
    "        \n",
    "        # Compute the spectrogram\n",
    "        spectrogram = np.abs(librosa.stft(data))\n",
    "        \n",
    "        # Split the spectrogram into 20 chunks\n",
    "        n_chunks = 20\n",
    "        chunks = np.array_split(spectrogram, n_chunks)\n",
    "        \n",
    "        # Append the chunks to the list of spectrograms\n",
    "        spectrograms.append(chunks)\n",
    "    \n",
    "    # Add the spectrogram chunks to the dataframe as new columns\n",
    "    for i in range(n_chunks):\n",
    "        dataframe[\"mean_spectrogram_\" + str(i)] = [np.mean(chunk[i]) for chunk in spectrograms]\n",
    "        dataframe[\"std_spectrogram_\" + str(i)] = [np.std(chunk[i]) for chunk in spectrograms]\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_spectrogram(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_spectrogram(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(dataframe):\n",
    "    mfcc_array = []\n",
    "    for data, rate in zip(dataframe['data'], dataframe['rate']):\n",
    "        # Load the audio file\n",
    "        mfcc = librosa.feature.mfcc(y=data, sr=rate, n_mfcc=30)        \n",
    "        # Split the spectrogram into 20 chunks\n",
    "        n_chunks = 20\n",
    "        chunks = np.array_split(mfcc, n_chunks)\n",
    "        \n",
    "        # Append the chunks to the list of spectrograms\n",
    "        mfcc_array.append(chunks)\n",
    "    \n",
    "    # Add the spectrogram chunks to the dataframe as new columns\n",
    "    for i in range(n_chunks):\n",
    "        dataframe[\"mean_mfcc_\" + str(i)] = [np.mean(chunk[i]) for chunk in mfcc_array]\n",
    "        dataframe[\"std_mfcc_\" + str(i)] = [np.std(chunk[i]) for chunk in mfcc_array]\n",
    "    return dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_mfcc(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_mfcc(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_feature(df)\n",
    "rmse_feature(df_eval)\n",
    "\n",
    "time_domain_1D(df,'rmse')\n",
    "time_domain_1D(df_eval,'rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sro_feature(df)\n",
    "sro_feature(df_eval)\n",
    "time_domain_1D(df,'spectral_rolloff')\n",
    "time_domain_1D(df_eval,'spectral_rolloff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_flatness(df)\n",
    "spectral_flatness(df_eval)\n",
    "time_domain_1D(df,'sf')\n",
    "time_domain_1D(df_eval,'sf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zcr_feature(df)\n",
    "zcr_feature(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chroma(dataframe):\n",
    "    chroma_array = []\n",
    "    for data, rate in zip(dataframe['data'], dataframe['rate']):\n",
    "        # Load the audio file\n",
    "        chroma = librosa.feature.chroma_stft(y=data, sr=rate, n_chroma=7)       \n",
    "        # Split the spectrogram into 20 chunks\n",
    "        n_chunks = 20\n",
    "        chunks = np.array_split(chroma, n_chunks)\n",
    "        \n",
    "        # Append the chunks to the list of spectrograms\n",
    "        chroma_array.append(chunks)\n",
    "    \n",
    "    # Add the spectrogram chunks to the dataframe as new columns\n",
    "    for i in range(n_chunks):\n",
    "        dataframe[\"mean_chroma_\" + str(i)] = [np.mean(chunk[i]) for chunk in chroma_array]\n",
    "        dataframe[\"std_chroma_\" + str(i)] = [np.std(chunk[i]) for chunk in chroma_array]\n",
    "    return dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_chroma(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_chroma(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tonnetz(dataframe):\n",
    "    tonnetz_array = []\n",
    "    for data, rate in zip(dataframe['data'], dataframe['rate']):\n",
    "        # Load the audio file\n",
    "        tonnetz = librosa.feature.tonnetz(y=data, sr=rate)\n",
    "        # Split the spectrogram into 20 chunks\n",
    "        n_chunks = 20\n",
    "        chunks = np.array_split(tonnetz, n_chunks)\n",
    "        \n",
    "        # Append the chunks to the list of spectrograms\n",
    "        tonnetz_array.append(chunks)\n",
    "    \n",
    "    # Add the spectrogram chunks to the dataframe as new columns\n",
    "    for i in range(n_chunks):\n",
    "        dataframe[\"mean_tonnetz_\" + str(i)] = [np.mean(chunk[i]) for chunk in tonnetz_array]\n",
    "        dataframe[\"std_tonnetz_\" + str(i)] = [np.std(chunk[i]) for chunk in tonnetz_array]\n",
    "    return dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_tonnetz(df)\n",
    "extract_tonnetz(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spectral_contrast(dataframe):\n",
    "    spectral_contrast_array = []\n",
    "    for data, rate in zip(dataframe['data'], dataframe['rate']):\n",
    "        # Load the audio file\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=data, sr=rate)\n",
    "        # Split the spectrogram into 20 chunks\n",
    "        n_chunks = 20\n",
    "        chunks = np.array_split(spectral_contrast, n_chunks)\n",
    "        \n",
    "        # Append the chunks to the list of spectrograms\n",
    "        spectral_contrast_array.append(chunks)\n",
    "    \n",
    "    # Add the spectrogram chunks to the dataframe as new columns\n",
    "    for i in range(n_chunks):\n",
    "        dataframe[\"mean_tspectral_contrast_\" + str(i)] = [np.mean(chunk[i]) for chunk in spectral_contrast_array]\n",
    "        dataframe[\"std_spectral_contrast_\" + str(i)] = [np.std(chunk[i]) for chunk in spectral_contrast_array]\n",
    "    return dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_spectral_contrast(df)\n",
    "extract_spectral_contrast(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.drop(columns=['Id','ageRange','data','speakerId','path', 'rate'],inplace= True)\n",
    "df.drop(columns=['Id','ageRange','data','speakerId','path', 'rate'],inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "X_eval = df_eval.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(columns=['rmse','sf','spectral_rolloff'], inplace=True)\n",
    "X_eval.drop(columns=['rmse','sf','spectral_rolloff'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.dropna(axis=1, how='all')\n",
    "X_eval = X_eval.dropna(axis=1, how='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X.to_csv('x.csv',index=False)\n",
    "\n",
    "        \n",
    "X_eval.to_csv('x_eval.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eval = pd.read_csv('x_eval.csv')\n",
    "X = pd.read_csv('x.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def calculate_class_counts(X, y):\n",
    "#     class_counts = {}\n",
    "#     for target in set(y):\n",
    "#         class_counts[target] = y.tolist().count(target)\n",
    "#     return class_counts\n",
    "\n",
    "# print(calculate_class_counts(X_new, y_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Create a copy of the dataframe\n",
    "X_scaled = X.copy()\n",
    "X_eval_scaled = X_eval.copy()\n",
    "\n",
    "# Extract the column names\n",
    "column_names = X.columns\n",
    "\n",
    "# Create the StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_scaled[column_names] = scaler.fit_transform(X[column_names])\n",
    "# X_eval_scaled[column_names] = scaler.transform(X_eval[column_names])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def select_top_n_features(X_train, X_test, threshold):\n",
    "    pca = PCA()\n",
    "    \n",
    "    # Fit PCA on the training data\n",
    "    pca.fit(X_train)\n",
    "    \n",
    "    # Get the variance ratio for each principal component\n",
    "    variance_ratio = pca.explained_variance_ratio_\n",
    "    n_components = len(variance_ratio)\n",
    "    \n",
    "    # Find the number of components that explain at least the threshold of variance\n",
    "    explained_variance = 0\n",
    "    for i, ratio in enumerate(variance_ratio):\n",
    "        explained_variance += ratio\n",
    "        if explained_variance >= threshold:\n",
    "            n_components = i + 1\n",
    "            break\n",
    "    print(f\"Number of components that explain at least {threshold} of variance : {n_components}\")\n",
    "    # Initialize the PCA object with the number of components\n",
    "    pca = PCA(n_components=n_components)\n",
    "    \n",
    "    # Fit PCA on the training data\n",
    "    pca.fit(X_train)\n",
    "    # Transform the training and test data to the first n principal components\n",
    "    X_train_top_n_features = pca.transform(X_train)\n",
    "    X_test_top_n_features = pca.transform(X_test)\n",
    "    #Return the top features and the variance ratio\n",
    "    return X_train_top_n_features, X_test_top_n_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_eval =select_top_n_features(X_scaled, X_eval_scaled, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def svm_model2(X_train, y_train, X_test):\n",
    "    clf = SVC(C = 100, gamma = 0.01, kernel = 'rbf')\n",
    "    # train the model on the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "    # predict the target values for the test data\n",
    "    # returning the y_predict\n",
    "    return clf.predict(X_test)\n",
    "y_pred = svm_model2(X, y, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_calculator(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encodes(df, column_name):\n",
    "    le = LabelEncoder()\n",
    "    label = le.fit_transform(df[column_name])\n",
    "    df.drop(column_name, axis=1, inplace=True)\n",
    "    df[column_name] = label\n",
    "\n",
    "label_encodes(X, 'Self-reported fluency level ')\n",
    "label_encodes(X, 'First Language spoken')\n",
    "label_encodes(X, 'Current language used for work/school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def svm_model2(X_train, y_train, X_test):\n",
    "    clf = SVC(kernel=\"rbf\",C=10)\n",
    "    # train the model on the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "    # predict the target values for the test data\n",
    "    # returning the y_predict\n",
    "    return clf.predict(X_test)\n",
    "y_pred = svm_model2(X_train, y_train, X_test)\n",
    "svm_accuracy = accuracy_calculator(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_decoded = label_encoder.inverse_transform(y_pred)\n",
    "pd.Series(y_decoded).to_csv('predictions.csv', index='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a list of all possible values for each attribute\n",
    "n_estimators = np.arange(10, 100, 10)\n",
    "max_depth = [None, 5, 10, 15, 20]\n",
    "min_samples_split = np.arange(2, 20, 2)\n",
    "min_samples_leaf = np.arange(1, 20, 2)\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "\n",
    "# Create a list of all possible combinations of the attributes\n",
    "combinations = [(a,b,c,d,e) for a in n_estimators for b in max_depth for c in min_samples_split for d in min_samples_leaf for e in max_features]\n",
    "\n",
    "# Select the first four combinations\n",
    "selected_combinations = combinations[:4]\n",
    "\n",
    "# Print the selected combinations\n",
    "print(selected_combinations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "accuracy = []\n",
    "\n",
    "# Generate some sample data\n",
    "\n",
    "for comb in combinations:\n",
    "\n",
    "    # Initialize the Random Forest classifier\n",
    "    rf = RandomForestClassifier(n_estimators = comb[0],max_depth=comb[1], min_samples_split = comb[2], min_samples_leaf=  comb[3], max_features= comb[4])\n",
    "\n",
    "    # Fit the classifier to the data\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on new data\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    # Print the accuracy of the model\n",
    "    print(comb)\n",
    "    print(\"Accuracy:\", rf.score(X_test, y_test))\n",
    "    accuracy.append((rf.score(X_test, y_test),comb))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_decoded = label_encoder.inverse_transform(y_pred)\n",
    "pd.Series(y_decoded).to_csv('predictions.csv', index='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (v3.11.1:a7a450f84a, Dec  6 2022, 15:24:06) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
