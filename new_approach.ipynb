{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from algorithms import *\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract development and evaulation\n",
    "df = pd.read_csv(\"dsl_data/development.csv\")\n",
    "df_eval = pd.read_csv(\"dsl_data/evaluation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = (df['Current language used for work/school'] == np.unique(df_eval['Current language used for work/school'].values)[0]) & (df['First Language spoken'] == np.unique(df_eval['First Language spoken'].values)[0]) & (df['Self-reported fluency level '] == np.unique(df_eval['Self-reported fluency level '].values)[0])\n",
    "df = df.loc[condition]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['intent'] = df['action']+ df['object']\n",
    "# encoding y\n",
    "dict = {}\n",
    "for i, el in enumerate(df['intent'].unique()):\n",
    "    dict[el] = i\n",
    "\n",
    "df['intent'] = df['intent'].apply(lambda x: dict[x])\n",
    "y = df['intent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eval = pd.read_csv('x_eval.csv')\n",
    "X = pd.read_csv('x.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping irrelevant columns\n",
    "df.drop(columns=[\"Self-reported fluency level \",\"First Language spoken\", \"Current language used for work/school\",\"action\",\"object\", \"intent\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping irrelevant columns\n",
    "df_eval.drop(columns=[\"Self-reported fluency level \",\"First Language spoken\", \"Current language used for work/school\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder(df, 'gender')\n",
    "label_encoder(df_eval, 'gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder(df, 'ageRange')\n",
    "label_encoder(df_eval, 'ageRange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_feature_extraction(df)\n",
    "audio_feature_extraction(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def extract_spectrogram(dataframe):\n",
    "    audio_paths = dataframe[\"path\"].values\n",
    "    spectrograms = []\n",
    "    for data in dataframe['data']:\n",
    "        \n",
    "        # Compute the spectrogram\n",
    "        spectrogram = np.abs(librosa.stft(data))\n",
    "        \n",
    "        # Split the spectrogram into 20 chunks\n",
    "        n_chunks = 20\n",
    "        chunks = np.array_split(spectrogram, n_chunks)\n",
    "        \n",
    "        # Append the chunks to the list of spectrograms\n",
    "        spectrograms.append(chunks)\n",
    "    \n",
    "    # Add the spectrogram chunks to the dataframe as new columns\n",
    "    for i in range(n_chunks):\n",
    "        dataframe[\"mean_spectrogram_\" + str(i)] = [np.mean(chunk[i]) for chunk in spectrograms]\n",
    "        dataframe[\"std_spectrogram_\" + str(i)] = [np.std(chunk[i]) for chunk in spectrograms]\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_spectrogram(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_spectrogram(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(dataframe):\n",
    "    mfcc_array = []\n",
    "    for data, rate in zip(dataframe['data'], dataframe['rate']):\n",
    "        # Load the audio file\n",
    "        mfcc = librosa.feature.mfcc(y=data, sr=rate, n_mfcc=30)        \n",
    "        # Split the spectrogram into 20 chunks\n",
    "        n_chunks = 20\n",
    "        chunks = np.array_split(mfcc, n_chunks)\n",
    "        \n",
    "        # Append the chunks to the list of spectrograms\n",
    "        mfcc_array.append(chunks)\n",
    "    \n",
    "    # Add the spectrogram chunks to the dataframe as new columns\n",
    "    for i in range(n_chunks):\n",
    "        dataframe[\"mean_mfcc_\" + str(i)] = [np.mean(chunk[i]) for chunk in mfcc_array]\n",
    "        dataframe[\"std_mfcc_\" + str(i)] = [np.std(chunk[i]) for chunk in mfcc_array]\n",
    "    return dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_mfcc(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_mfcc(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.drop(columns=['Id','ageRange','data','speakerId','path', 'rate'],inplace= True)\n",
    "df.drop(columns=['Id','ageRange','data','speakerId','path', 'rate'],inplace= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "X_eval = df_eval.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zcr_feature(df)\n",
    "zcr_feature(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X.to_csv('x.csv',index=False)\n",
    "\n",
    "\n",
    "X_eval.to_csv('x_eval.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eval = pd.read_csv('x_eval.csv')\n",
    "X = pd.read_csv('x.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X)\n",
    "scaler.fit(X_eval)\n",
    "\n",
    "X = scaler.transform(X)\n",
    "X_eval = scaler.transform(X_eval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf'], 'gamma': [0.01, 0.1, 1, 10]}\n",
    "\n",
    "# Initialize the SVC\n",
    "svc = SVC()\n",
    "\n",
    "# Initialize the GridSearchCV\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "y = np.array([0, 1, 2])\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf'], 'gamma': [0.1, 1, 10]}\n",
    "\n",
    "# Initialize the SVC\n",
    "svc = SVC()\n",
    "\n",
    "# Initialize the KFold\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Initialize the GridSearchCV\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=kf)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'class_weight' parameter of SVC must be a str among {'balanced'}, an instance of 'dict' or None. Got array([2.70670996, 1.24634551, 1.75714286, 0.59382667, 0.5337934 ,\n       1.16001855, 1.19095238]) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m clf \u001b[39m=\u001b[39m SVC(gamma\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, kernel\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrbf\u001b[39m\u001b[39m'\u001b[39m, C\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, class_weight\u001b[39m=\u001b[39mclass_weights)\n\u001b[1;32m     12\u001b[0m \u001b[39m# Fit the classifier on the training data\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m clf\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n",
      "File \u001b[0;32m~/Dev/POLITO/DSL-Project/NLP-Intent-detection/venv/lib/python3.10/site-packages/sklearn/svm/_base.py:180\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    148\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit the SVM model according to the given training data.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \n\u001b[1;32m    150\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39m    matrices as input.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_params()\n\u001b[1;32m    182\u001b[0m     rnd \u001b[39m=\u001b[39m check_random_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n\u001b[1;32m    184\u001b[0m     sparse \u001b[39m=\u001b[39m sp\u001b[39m.\u001b[39misspmatrix(X)\n",
      "File \u001b[0;32m~/Dev/POLITO/DSL-Project/NLP-Intent-detection/venv/lib/python3.10/site-packages/sklearn/base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_params\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    574\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \n\u001b[1;32m    576\u001b[0m \u001b[39m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[39m    accepted constraints.\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 581\u001b[0m     validate_parameter_constraints(\n\u001b[1;32m    582\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parameter_constraints,\n\u001b[1;32m    583\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_params(deep\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m    584\u001b[0m         caller_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m,\n\u001b[1;32m    585\u001b[0m     )\n",
      "File \u001b[0;32m~/Dev/POLITO/DSL-Project/NLP-Intent-detection/venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:97\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     constraints_str \u001b[39m=\u001b[39m (\n\u001b[1;32m     93\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mstr\u001b[39m(c)\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39mconstraints[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]])\u001b[39m}\u001b[39;00m\u001b[39m or\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m     )\n\u001b[0;32m---> 97\u001b[0m \u001b[39mraise\u001b[39;00m InvalidParameterError(\n\u001b[1;32m     98\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m{\u001b[39;00mparam_name\u001b[39m!r}\u001b[39;00m\u001b[39m parameter of \u001b[39m\u001b[39m{\u001b[39;00mcaller_name\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints_str\u001b[39m}\u001b[39;00m\u001b[39m. Got \u001b[39m\u001b[39m{\u001b[39;00mparam_val\u001b[39m!r}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m )\n",
      "\u001b[0;31mInvalidParameterError\u001b[0m: The 'class_weight' parameter of SVC must be a str among {'balanced'}, an instance of 'dict' or None. Got array([2.70670996, 1.24634551, 1.75714286, 0.59382667, 0.5337934 ,\n       1.16001855, 1.19095238]) instead."
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "\n",
    "# Create the SVC classifier with class weights\n",
    "clf = SVC(gamma=0.01, kernel='rbf', C=10, class_weight=class_weights)\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4152452025586354"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_calculator(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_mapping = np.vectorize(lambda x: next(key for key, value in dict.items() if value == x))\n",
    "keys_arr = key_mapping(y_pred)\n",
    "pd.Series(keys_arr).to_csv('predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09ee11da7dc62f10eaf9df9c4559184832adcd88d836286cbf9804f9d006cf55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
