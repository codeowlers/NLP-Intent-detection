{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from algorithms import label_encode_columns, svm_model, accuracy_calculator, random_forest_model, array_column_spread, one_hot_encode_columns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dsl_data/development.csv\")\n",
    "df_eval = pd.read_csv(\"dsl_data/evaluation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['intent'] = df['action']+df['object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y encoded done\n",
    "dict = {}\n",
    "\n",
    "for i, el in enumerate(df['intent'].unique()):\n",
    "    dict[el] = i\n",
    "\n",
    "df['intent'] = df['intent'].apply(lambda x: dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Self-reported fluency level \",\"First Language spoken\", \"Current language used for work/school\", \"gender\"]\n",
    "\n",
    "for column in columns:\n",
    "    encoder = OneHotEncoder()\n",
    "    encoded_data = encoder.fit_transform(df[[column]])\n",
    "    feature_names = encoder.get_feature_names_out([column])\n",
    "    encoded_df = pd.DataFrame(encoded_data.toarray(), columns=feature_names)\n",
    "    df.drop(column, axis=1, inplace=True)\n",
    "    df = pd.concat([df, encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df['ageRange'])\n",
    "df.drop('ageRange', axis=1, inplace=True)\n",
    "df['ageRange'] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_feature_extraction(df):\n",
    "        data_array= []\n",
    "        rate_array = []\n",
    "        for audio in df['path']:\n",
    "                data, rate = librosa.load(audio)\n",
    "                data_array.append(data)\n",
    "                rate_array.append(rate)\n",
    "\n",
    "        df['data'] = data_array\n",
    "        df['rate'] = rate_array\n",
    "audio_feature_extraction(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def time_domain(df, column):\n",
    "    df[f'{column}_mean'] = df[column].apply(lambda x: np.mean(x,axis=1))\n",
    "    df[f'{column}_min'] = df[column].apply(lambda x: np.min(x,axis=1))\n",
    "    df[f'{column}_max'] = df[column].apply(lambda x: np.max(x,axis=1))\n",
    "    df[f'{column}_skew'] = df[column].apply(lambda x: skew(x,axis=1))\n",
    "    df[f'{column}_kurtosis'] = df[column].apply(lambda x: kurtosis(x,axis=1))\n",
    "    df[f'{column}_std'] = df[column].apply(lambda x: np.std(x,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chroma_feature(df):\n",
    "    chroma_array = []\n",
    "    for data, rate in zip(df['data'], df['rate']):\n",
    "        chroma = librosa.feature.chroma_stft(y=data, sr=rate, n_chroma=7)\n",
    "        chroma_array.append(chroma)\n",
    "    df['chroma'] = chroma_array\n",
    "\n",
    "chroma_feature(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_domain(df,'chroma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = ['chroma_mean','chroma_min','chroma_max','chroma_std','chroma_kurtosis','chroma_skew']\n",
    "for i in arr:\n",
    "    array_column_spread(df,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tonnetz_feature(df):\n",
    "    tonnetz_array = []\n",
    "    for data, rate in zip(df['data'], df['rate']):\n",
    "        tonnetz = librosa.feature.tonnetz(y=data, sr=rate)\n",
    "        tonnetz_array.append(tonnetz)\n",
    "    df['tonnetz'] = tonnetz_array\n",
    "tonnetz_feature(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_domain(df,'tonnetz')\n",
    "arr = ['tonnetz_mean','tonnetz_min','tonnetz_max','tonnetz_std','tonnetz_kurtosis','tonnetz_skew']\n",
    "for i in arr:\n",
    "    array_column_spread(df,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# down to 4 features\n",
    "def spectral_contrast(df):\n",
    "    spectral_contrast_array = []\n",
    "    for data, rate in zip(df['data'], df['rate']):\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=data, sr=rate)\n",
    "        spectral_contrast_array.append(spectral_contrast)\n",
    "    df['spectral_contrast'] = spectral_contrast_array    \n",
    "spectral_contrast(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_domain(df,'spectral_contrast')\n",
    "arr = ['spectral_contrast_mean','spectral_contrast_min','spectral_contrast_max','spectral_contrast_std','spectral_contrast_kurtosis','spectral_contrast_skew']\n",
    "for i in arr:\n",
    "    array_column_spread(df,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_feature(df):\n",
    "    # create an empty list to store the RMSE values\n",
    "    rmse_list = []\n",
    "\n",
    "    # iterate through the audio files in the dataset\n",
    "    for data, rate in zip(df['data'], df['rate']):\n",
    "        # calculate the root mean square energy\n",
    "        rmse = librosa.feature.rms(y=data)\n",
    "        # append the rmse mean to the rmse_list\n",
    "        rmse_list.append(rmse[0])\n",
    "\n",
    "    # add the rmse_list as a new column to the dataframe\n",
    "    df['rmse'] = rmse_list\n",
    "\n",
    "rmse_feature(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_domain2(df, column):\n",
    "    df[f'{column}_mean'] = df[column].apply(lambda x: np.mean(x))\n",
    "    df[f'{column}_min'] = df[column].apply(lambda x: np.min(x))\n",
    "    df[f'{column}_max'] = df[column].apply(lambda x: np.max(x))\n",
    "    df[f'{column}_skew'] = df[column].apply(lambda x: skew(x))\n",
    "    df[f'{column}_kurtosis'] = df[column].apply(lambda x: kurtosis(x))\n",
    "    df[f'{column}_std'] = df[column].apply(lambda x: np.std(x))\n",
    "time_domain2(df,'rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_flatness(df):\n",
    "    # create an empty list to store the SF values\n",
    "    sf_list = []\n",
    "\n",
    "    # iterate through the audio files in the dataset\n",
    "    for data, rate in zip(df['data'], df['rate']):\n",
    "        # calculate the spectral flatness\n",
    "        sf = librosa.feature.spectral_flatness(y=data)\n",
    "        # append the SF mean to the sf_list\n",
    "        sf_list.append(sf[0])\n",
    "\n",
    "    # add the sf_list as a new column to the dataframe\n",
    "    df['sf'] = sf_list\n",
    "spectral_flatness(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_domain2(df,'sf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sro_feature(df):\n",
    "    # Create an empty list to store the spectral roll-off values\n",
    "    spectral_rolloff_array = []\n",
    "\n",
    "    for data, rate in zip(df['data'], df['rate']):\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=data, sr=rate)\n",
    "        spectral_rolloff_array.append(spectral_rolloff[0])\n",
    "\n",
    "    # Add the spectral roll-off values to the dataframe as a new column\n",
    "    df['spectral_rolloff'] = spectral_rolloff_array\n",
    "sro_feature(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_domain2(df,'spectral_rolloff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zcr_feature(df):\n",
    "    # Create arrays to store the zero-crossing rate values\n",
    "    zero_crossing_rate_array = []\n",
    "\n",
    "    for data in df['data']:\n",
    "        \n",
    "        # Compute the zero-crossing rate for the current audio file\n",
    "        zero_crossing_rate = sum(librosa.zero_crossings(data))\n",
    "        # Append the zero-crossing rate to the zero_crossing_rate_array\n",
    "        zero_crossing_rate_array.append(zero_crossing_rate)\n",
    "\n",
    "    # Add the zero-crossing rate arrays as new columns in the dataframe\n",
    "    df['zero_crossing_rate'] = zero_crossing_rate_array\n",
    "zcr_feature(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfcc_feature(df):\n",
    "    # Create arrays to store the mfcc rate values\n",
    "    mfcc_array = []\n",
    "\n",
    "    for data, rate in zip(df['data'], df['rate']):\n",
    "        \n",
    "        # Compute the mfccs for the current audio file\n",
    "        mfcc = librosa.feature.mfcc(y=data, sr=rate, n_mfcc=30)\n",
    "        mfcc_array.append(mfcc)\n",
    "\n",
    "    # Add the mfcc as a new column in the dataframe\n",
    "    df['mfcc'] = mfcc_array\n",
    "mfcc_feature(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_domain(df,'mfcc')\n",
    "arr = ['mfcc_mean','mfcc_min','mfcc_max','mfcc_std','mfcc_kurtosis','mfcc_skew']\n",
    "for i in arr:\n",
    "    array_column_spread(df,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "\n",
    "# Iterate over each column in the DataFrame\n",
    "for col in df2.columns:\n",
    "    # Check if the column contains 1D or 2D arrays\n",
    "    if isinstance(df2[col].values[0], (list, np.ndarray)) and len(df2[col].values[0]) > 1:\n",
    "        # Drop the column if it contains 1D or 2D arrays\n",
    "        df2.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = ['Id', 'path', 'speakerId', 'action', 'object', 'intent',\n",
    "       'Self-reported fluency level _advanced',\n",
    "       'Self-reported fluency level _basic',\n",
    "       'Self-reported fluency level _intermediate',\n",
    "       'Self-reported fluency level _native',\n",
    "       'First Language spoken_English (Canada)',\n",
    "       'First Language spoken_English (United States)',\n",
    "       'First Language spoken_French (Canada)',\n",
    "       'First Language spoken_Spanish (Venezuela)',\n",
    "       'First Language spoken_Telugu',\n",
    "       'Current language used for work/school_English (Australia)',\n",
    "       'Current language used for work/school_English (Canada)',\n",
    "       'Current language used for work/school_English (United States)',\n",
    "       'Current language used for work/school_Spanish (Venezuela)',\n",
    "       'gender_female', 'gender_male', 'ageRange', 'data', 'rate',]\n",
    "# arr2 = df2.columns.to_list()\n",
    "# arr3 = [x for x in arr2 if x not in arr1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(columns=['Id', 'path', 'speakerId', 'action', 'object', 'intent'], inplace=True)\n",
    "\n",
    "X.to_csv('file_name.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# import pandas as pd\n",
    "\n",
    "# # Assuming your dataframe is named 'df'\n",
    "\n",
    "# # Create an instance of PCA\n",
    "# pca = PCA()\n",
    "\n",
    "# # Fit PCA on the data\n",
    "# pca.fit(X)\n",
    "\n",
    "# # Get the explained variance ratio of each principal component\n",
    "# var_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# # Get the cumulative explained variance ratio\n",
    "# cum_var_ratio = np.cumsum(var_ratio)\n",
    "\n",
    "# # Plot the explained variance ratio\n",
    "# plt.bar(range(1, len(var_ratio)+1), var_ratio, alpha=0.5, align='center', label='individual explained variance')\n",
    "# plt.step(range(1, len(cum_var_ratio)+1), cum_var_ratio, where='mid', label='cumulative explained variance')\n",
    "# plt.ylabel('Explained variance ratio')\n",
    "# plt.xlabel('Principal component index')\n",
    "# plt.legend(loc='best')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # Create an instance of PCA, specifying the number of components to keep\n",
    "# pca = PCA(n_components=75)\n",
    "\n",
    "# # Fit PCA on the data\n",
    "# df_pca = pca.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['intent'].values\n",
    "X = pd.read_csv(\"file_name.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# def svm_model2(X_train, y_train, X_test):\n",
    "#     clf = SVC()\n",
    "#     # train the model on the training data\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     # predict the target values for the test data\n",
    "#     # returning the y_predict\n",
    "#     return clf.predict(X_test)\n",
    "# y_pred = svm_model2(X_train, y_train, X_test)\n",
    "# svm_accuracy = accuracy_calculator(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "# # shuffle the data before performing k-fold cross validation\n",
    "# X_shuffled, y_shuffled = shuffle(X, y)\n",
    "\n",
    "# # perform k-fold cross validation with 5 folds\n",
    "# scores = cross_val_score(clf, X_shuffled, y_shuffled, cv=5)\n",
    "\n",
    "# # calculate the mean accuracy of the model across all folds\n",
    "# accuracy = np.mean(scores)\n",
    "# print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance:  [2.19855067e-01 7.25938116e-02 4.68347961e-02 3.55297043e-02\n",
      " 2.69045925e-02 2.30217061e-02 2.07242683e-02 1.74690633e-02\n",
      " 1.43397480e-02 1.41129598e-02 1.32533767e-02 1.27668932e-02\n",
      " 1.19028809e-02 1.11236775e-02 1.03485542e-02 9.78553025e-03\n",
      " 9.63189507e-03 9.39479012e-03 9.13552142e-03 8.53246062e-03\n",
      " 8.27115772e-03 7.88151324e-03 7.60416874e-03 7.43088708e-03\n",
      " 7.11269620e-03 6.93390211e-03 6.77288310e-03 6.71830806e-03\n",
      " 6.44756995e-03 6.18289453e-03 6.01555285e-03 5.97587509e-03\n",
      " 5.76843886e-03 5.69087325e-03 5.46290365e-03 5.27673038e-03\n",
      " 5.20925122e-03 5.16640895e-03 4.91060784e-03 4.85717976e-03\n",
      " 4.72561326e-03 4.48772272e-03 4.41103507e-03 4.31093237e-03\n",
      " 4.27507280e-03 4.15577497e-03 4.06733817e-03 3.88674683e-03\n",
      " 3.80762373e-03 3.77580116e-03 3.67576642e-03 3.59462925e-03\n",
      " 3.52735108e-03 3.48680748e-03 3.41376735e-03 3.27186405e-03\n",
      " 3.25556938e-03 3.20916879e-03 3.16868948e-03 3.06601605e-03\n",
      " 3.02716291e-03 2.92481878e-03 2.88333475e-03 2.85777625e-03\n",
      " 2.81829934e-03 2.79548626e-03 2.73911592e-03 2.67845054e-03\n",
      " 2.63199374e-03 2.60427167e-03 2.58859673e-03 2.53023510e-03\n",
      " 2.51690330e-03 2.48919078e-03 2.39360096e-03 2.37948486e-03\n",
      " 2.35482070e-03 2.30736697e-03 2.28889491e-03 2.26122574e-03\n",
      " 2.23217015e-03 2.21980584e-03 2.21754038e-03 2.17088187e-03\n",
      " 2.15905171e-03 2.11455807e-03 2.08737809e-03 2.05974718e-03\n",
      " 2.03848749e-03 2.01090226e-03 1.97999730e-03 1.95968354e-03\n",
      " 1.95517139e-03 1.92068180e-03 1.88741123e-03 1.86088764e-03\n",
      " 1.82441473e-03 1.80569053e-03 1.78017623e-03 1.75473912e-03\n",
      " 1.73657073e-03 1.70079733e-03 1.66711165e-03 1.65910680e-03\n",
      " 1.64975882e-03 1.63738701e-03 1.59778507e-03 1.56903410e-03\n",
      " 1.52220688e-03 1.51623003e-03 1.50260365e-03 1.49331156e-03\n",
      " 1.47089580e-03 1.43911464e-03 1.42368910e-03 1.39301122e-03\n",
      " 1.36931569e-03 1.35404372e-03 1.35183586e-03 1.32275766e-03\n",
      " 1.30105058e-03 1.29440157e-03 1.28380336e-03 1.26487538e-03\n",
      " 1.24603850e-03 1.23317652e-03 1.21577882e-03 1.19623909e-03\n",
      " 1.19023210e-03 1.17347847e-03 1.16486880e-03 1.11525911e-03\n",
      " 1.11059162e-03 1.09648525e-03 1.07335078e-03 1.05725275e-03\n",
      " 1.03626877e-03 1.02412662e-03 1.01856171e-03 1.00732225e-03\n",
      " 9.94106131e-04 9.91200661e-04 9.55513601e-04 9.41189746e-04\n",
      " 9.29159961e-04 9.06513019e-04 8.98481574e-04 8.75334510e-04\n",
      " 8.60320835e-04 8.52079955e-04 8.25523601e-04 8.11346987e-04\n",
      " 8.00692000e-04 7.93145452e-04 7.73984178e-04 7.58700706e-04\n",
      " 7.42335903e-04 7.29954040e-04 7.23187477e-04 7.12642217e-04\n",
      " 6.95133882e-04 6.86310471e-04 6.70453866e-04 6.64167036e-04\n",
      " 6.56535147e-04 6.46871734e-04 6.27797564e-04 6.19134343e-04\n",
      " 6.14419564e-04 5.99190102e-04 5.86811943e-04 5.76487024e-04\n",
      " 5.72588903e-04 5.50048156e-04 5.41136151e-04 5.33476257e-04\n",
      " 5.19153257e-04 5.09271119e-04 4.93396033e-04 4.89519506e-04\n",
      " 4.78583064e-04 4.68372209e-04 4.58823518e-04 4.53469494e-04\n",
      " 4.40310216e-04 4.37163898e-04 4.25322588e-04 4.18582310e-04\n",
      " 4.14176368e-04 4.08700683e-04 4.02319774e-04 3.93283826e-04\n",
      " 3.82179805e-04 3.76631286e-04 3.64208134e-04 3.58699992e-04\n",
      " 3.54054689e-04 3.49623563e-04 3.47301258e-04 3.44819531e-04\n",
      " 3.39150040e-04 3.35232222e-04 3.25058564e-04 3.19187707e-04\n",
      " 3.12021193e-04 3.08360926e-04 3.05010959e-04 3.02213282e-04\n",
      " 2.96529855e-04 2.92695885e-04 2.87686564e-04 2.83709369e-04\n",
      " 2.77824652e-04 2.72962889e-04 2.69449113e-04 2.64808123e-04\n",
      " 2.57959664e-04 2.56400008e-04 2.53647949e-04 2.42287593e-04\n",
      " 2.41062528e-04 2.37322931e-04 2.32503155e-04 2.30589843e-04\n",
      " 2.27753979e-04 2.26029910e-04 2.23174945e-04 2.18289230e-04\n",
      " 2.16689021e-04 2.11540092e-04 2.10230347e-04 2.08698378e-04\n",
      " 2.05962626e-04 2.05044576e-04 2.01132255e-04 1.96190080e-04\n",
      " 1.94432408e-04 1.93867643e-04 1.89553646e-04 1.88220095e-04\n",
      " 1.83496032e-04 1.80910425e-04 1.79539806e-04 1.78782830e-04\n",
      " 1.75288207e-04 1.75060433e-04 1.71616658e-04 1.68053569e-04\n",
      " 1.66973659e-04 1.63911509e-04 1.63115611e-04 1.62714351e-04\n",
      " 1.60735394e-04 1.59111234e-04 1.56999069e-04 1.55389463e-04\n",
      " 1.52770069e-04 1.51532382e-04 1.47527687e-04 1.46127697e-04\n",
      " 1.45815568e-04 1.43672125e-04 1.43174241e-04 1.39368517e-04\n",
      " 1.38701806e-04 1.34621656e-04 1.33917924e-04 1.32381449e-04\n",
      " 1.31005247e-04 1.29509632e-04 1.27190710e-04 1.25451811e-04\n",
      " 1.24665013e-04 1.22228364e-04 1.21277083e-04 1.18223014e-04\n",
      " 1.17568522e-04 1.15302711e-04 1.12797241e-04 1.11238163e-04\n",
      " 1.10726704e-04 1.08959080e-04 1.08013993e-04 1.03716892e-04\n",
      " 1.03318730e-04 1.00892995e-04 1.00024228e-04 9.77446881e-05\n",
      " 9.74354432e-05 9.52764918e-05 9.39755073e-05 9.05513326e-05\n",
      " 8.86506239e-05 8.77050566e-05 8.63071664e-05 8.50623038e-05\n",
      " 8.25603809e-05 8.10179821e-05 8.01486068e-05 7.70140689e-05\n",
      " 7.55721606e-05 7.36244182e-05 7.17725771e-05 7.06059107e-05\n",
      " 6.79791721e-05 6.72888419e-05 6.45976828e-05 6.19334669e-05\n",
      " 5.99208928e-05 5.85290674e-05 5.46233287e-05 5.38653005e-05\n",
      " 5.31377549e-05 4.95656797e-05 4.60312079e-05 4.34616791e-05\n",
      " 4.28113845e-05 4.07129484e-05 3.92289419e-05 3.02300812e-05\n",
      " 2.93939649e-05 2.77617932e-05 2.11299009e-05 2.01407153e-05\n",
      " 1.93125578e-05 1.77086856e-05 1.31992826e-05 4.06905231e-08\n",
      " 5.49294828e-29 2.24309045e-29 7.75221285e-30 2.37712846e-30\n",
      " 2.29182272e-31 4.32903958e-32 1.10150356e-33 1.10150356e-33]\n",
      "Cumulative Explained Variance:  [0.21985507 0.29244888 0.33928367 0.37481338 0.40171797 0.42473968\n",
      " 0.44546395 0.46293301 0.47727276 0.49138572 0.50463909 0.51740599\n",
      " 0.52930887 0.54043254 0.5507811  0.56056663 0.57019852 0.57959331\n",
      " 0.58872884 0.5972613  0.60553245 0.61341397 0.62101814 0.62844902\n",
      " 0.63556172 0.64249562 0.6492685  0.65598681 0.66243438 0.66861728\n",
      " 0.67463283 0.68060871 0.68637714 0.69206802 0.69753092 0.70280765\n",
      " 0.7080169  0.71318331 0.71809392 0.7229511  0.72767671 0.73216444\n",
      " 0.73657547 0.7408864  0.74516148 0.74931725 0.75338459 0.75727134\n",
      " 0.76107896 0.76485476 0.76853053 0.77212516 0.77565251 0.77913931\n",
      " 0.78255308 0.78582495 0.78908052 0.79228968 0.79545837 0.79852439\n",
      " 0.80155155 0.80447637 0.80735971 0.81021748 0.81303578 0.81583127\n",
      " 0.81857038 0.82124883 0.82388083 0.8264851  0.8290737  0.83160393\n",
      " 0.83412083 0.83661003 0.83900363 0.84138311 0.84373793 0.8460453\n",
      " 0.84833419 0.85059542 0.85282759 0.8550474  0.85726494 0.85943582\n",
      " 0.86159487 0.86370943 0.86579681 0.86785655 0.86989504 0.87190594\n",
      " 0.87388594 0.87584562 0.8778008  0.87972148 0.88160889 0.88346978\n",
      " 0.88529419 0.88709988 0.88888006 0.8906348  0.89237137 0.89407216\n",
      " 0.89573928 0.89739838 0.89904814 0.90068553 0.90228331 0.90385235\n",
      " 0.90537455 0.90689078 0.90839339 0.9098867  0.9113576  0.91279671\n",
      " 0.9142204  0.91561341 0.91698273 0.91833677 0.91968861 0.92101136\n",
      " 0.92231241 0.92360682 0.92489062 0.92615549 0.92740153 0.92863471\n",
      " 0.92985049 0.93104673 0.93223696 0.93341044 0.93457531 0.93569057\n",
      " 0.93680116 0.93789764 0.93897099 0.94002825 0.94106452 0.94208864\n",
      " 0.9431072  0.94411453 0.94510863 0.94609983 0.94705535 0.94799654\n",
      " 0.9489257  0.94983221 0.95073069 0.95160602 0.95246635 0.95331843\n",
      " 0.95414395 0.9549553  0.95575599 0.95654913 0.95732312 0.95808182\n",
      " 0.95882415 0.95955411 0.9602773  0.96098994 0.96168507 0.96237138\n",
      " 0.96304184 0.963706   0.96436254 0.96500941 0.96563721 0.96625634\n",
      " 0.96687076 0.96746995 0.96805676 0.96863325 0.96920584 0.96975589\n",
      " 0.97029702 0.9708305  0.97134965 0.97185892 0.97235232 0.97284184\n",
      " 0.97332042 0.9737888  0.97424762 0.97470109 0.9751414  0.97557856\n",
      " 0.97600389 0.97642247 0.97683664 0.97724534 0.97764766 0.97804095\n",
      " 0.97842313 0.97879976 0.97916397 0.97952267 0.97987672 0.98022635\n",
      " 0.98057365 0.98091847 0.98125762 0.98159285 0.98191791 0.9822371\n",
      " 0.98254912 0.98285748 0.98316249 0.9834647  0.98376123 0.98405393\n",
      " 0.98434161 0.98462532 0.98490315 0.98517611 0.98544556 0.98571037\n",
      " 0.98596833 0.98622473 0.98647838 0.98672066 0.98696173 0.98719905\n",
      " 0.98743155 0.98766214 0.9878899  0.98811593 0.9883391  0.98855739\n",
      " 0.98877408 0.98898562 0.98919585 0.98940455 0.98961051 0.98981555\n",
      " 0.99001669 0.99021288 0.99040731 0.99060118 0.99079073 0.99097895\n",
      " 0.99116245 0.99134336 0.9915229  0.99170168 0.99187697 0.99205203\n",
      " 0.99222365 0.9923917  0.99255867 0.99272258 0.9928857  0.99304841\n",
      " 0.99320915 0.99336826 0.99352526 0.99368065 0.99383342 0.99398495\n",
      " 0.99413248 0.99427861 0.99442442 0.99456809 0.99471127 0.99485064\n",
      " 0.99498934 0.99512396 0.99525788 0.99539026 0.99552127 0.99565078\n",
      " 0.99577797 0.99590342 0.99602808 0.99615031 0.99627159 0.99638981\n",
      " 0.99650738 0.99662268 0.99673548 0.99684672 0.99695744 0.9970664\n",
      " 0.99717442 0.99727813 0.99738145 0.99748235 0.99758237 0.99768011\n",
      " 0.99777755 0.99787283 0.9979668  0.99805735 0.998146   0.99823371\n",
      " 0.99832002 0.99840508 0.99848764 0.99856866 0.99864881 0.99872582\n",
      " 0.99880139 0.99887502 0.99894679 0.99901739 0.99908537 0.99915266\n",
      " 0.99921726 0.99927919 0.99933912 0.99939764 0.99945227 0.99950613\n",
      " 0.99955927 0.99960884 0.99965487 0.99969833 0.99974114 0.99978185\n",
      " 0.99982108 0.99985131 0.99988071 0.99990847 0.9999296  0.99994974\n",
      " 0.99996905 0.99998676 0.99999996 1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.        ]\n",
      "Top n Features:  ['mfcc_kurtosis_1', 'gender_male', 'mfcc_mean_23', 'mfcc_std_13', 'spectral_contrast_mean_4', 'First Language spoken_French (Canada)', 'mfcc_max_5', 'mfcc_mean_28', 'mfcc_max_0', 'mfcc_max_22', 'gender_female', 'mfcc_max_20', 'chroma_min_6', 'mfcc_mean_12', 'Current language used for work/school_English (United States)', 'tonnetz_mean_5', 'mfcc_mean_2', 'mfcc_min_17', 'tonnetz_skew_2', 'tonnetz_skew_0', 'tonnetz_skew_3', 'tonnetz_mean_3', 'mfcc_std_4', 'Self-reported fluency level _intermediate', 'tonnetz_mean_1', 'mfcc_max_10', 'Self-reported fluency level _intermediate', 'mfcc_min_28', 'mfcc_min_11', 'mfcc_max_18', 'First Language spoken_English (Canada)', 'tonnetz_mean_0', 'mfcc_min_7', 'Current language used for work/school_English (Australia)', 'Current language used for work/school_English (Australia)', 'Current language used for work/school_English (Australia)', 'tonnetz_skew_5', 'mfcc_std_13', 'chroma_std_1', 'Self-reported fluency level _basic', 'tonnetz_skew_2', 'mfcc_skew_7', 'tonnetz_skew_3', 'chroma_max_4', 'sf_kurtosis', 'Self-reported fluency level _basic', 'mfcc_skew_28', 'mfcc_skew_27', 'chroma_max_2', 'mfcc_min_16', 'mfcc_max_9', 'rmse_min', 'sf_kurtosis', 'chroma_min_5', 'First Language spoken_Telugu', 'sf_min', 'Self-reported fluency level _basic', 'sf_min', 'chroma_max_0', 'chroma_max_2', 'chroma_max_2', 'chroma_max_6', 'mfcc_skew_5', 'Self-reported fluency level _basic', 'chroma_max_0', 'Self-reported fluency level _basic', 'sf_min', 'mfcc_std_10', 'chroma_max_0', 'mfcc_skew_19', 'mfcc_min_27', 'mfcc_min_29', 'sf_min', 'tonnetz_std_2', 'chroma_max_1', 'sf_min', 'tonnetz_std_2', 'chroma_max_5', 'tonnetz_mean_4', 'mfcc_std_7', 'mfcc_max_18', 'tonnetz_skew_3', 'spectral_contrast_max_2', 'chroma_max_2', 'mfcc_std_16', 'mfcc_skew_16', 'mfcc_max_11', 'tonnetz_std_4', 'spectral_contrast_min_5', 'spectral_contrast_min_0', 'mfcc_std_18', 'mfcc_skew_4', 'mfcc_skew_7', 'mfcc_skew_12', 'mfcc_max_9', 'mfcc_max_6', 'chroma_max_5', 'mfcc_std_19', 'mfcc_skew_4', 'chroma_std_6', 'chroma_std_4', 'chroma_std_0', 'chroma_min_4', 'tonnetz_skew_2', 'mfcc_max_6', 'chroma_std_6']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Initialize PCA\n",
    "pca = PCA()\n",
    "\n",
    "# Fit PCA to the scaled training data\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# Get the explained variance ratio\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Calculate the cumulative explained variance\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Print the explained variance and cumulative explained variance\n",
    "print(\"Explained Variance: \", explained_variance)\n",
    "print(\"Cumulative Explained Variance: \", cumulative_explained_variance)\n",
    "\n",
    "# Determine the number of components to keep\n",
    "n_components = np.argmax(cumulative_explained_variance >= 0.9) + 1\n",
    "\n",
    "# Re-initialize PCA with the number of components to keep\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# Fit PCA to the scaled training data\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# Get the indices of the top n features\n",
    "feature_indices = np.argmax(pca.components_, axis=1)\n",
    "\n",
    "# Print out the top n features\n",
    "top_features = [X.columns[i] for i in feature_indices]\n",
    "print(\"Top n Features: \", top_features)\n",
    "\n",
    "# Transform the test set\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Create a new SVM model\n",
    "svm = SVC()\n",
    "\n",
    "# Use grid search to find the optimal parameters\n",
    "param_grid = {'kernel': ['linear', 'rbf'], 'C': [0.1, 1, 10], 'gamma': [0.01, 0.1, 1]}\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5)\n",
    "grid_search.fit(X_train_pca, y_train)\n",
    "\n",
    "# Print out the best parameters\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "\n",
    "\n",
    "# evaluate the best model on the test data\n",
    "best_model = grid_search.best_estimator_\n",
    "best_score = best_model.score(X_test_pca, y_test)\n",
    "print(\"Best Score on Test Data: \", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(**best_params)\n",
    "svm_model.fit(X_train_pca[:,top_80_features], y_train)\n",
    "\n",
    "# evaluate the model with test data\n",
    "test_score = svm_model.score(X_test_pca[:,top_80_features], y_test)\n",
    "print(\"Test Score: \", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Use grid search to find the optimal parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "param_grid = {'kernel': ['linear', 'rbf'], 'C': [0.1, 1, 10], 'gamma': [0.01, 0.1, 1]}\n",
    "svm = SVC()\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print out the best parameters\n",
    "print(\"Best Parameters: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Use cross-validation to evaluate performance\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(grid_search.best_estimator_, X_train, y_train, cv=5)\n",
    "\n",
    "# Print out the best score\n",
    "print(\"Best Score: \", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Select top n features\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "selector = SelectKBest(f_classif, k=80)\n",
    "selector.fit(X_train, y_train)\n",
    "X_train_best = selector.transform(X_train)\n",
    "X_test_best = selector.transform(X_test)\n",
    "# Get the indices of the top n features\n",
    "feature_indices = selector.get_support(indices=True)\n",
    "\n",
    "# Print out the best features\n",
    "print(\"Best Features: \", [X.columns[i] for i in feature_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Re-train model with optimal parameters and top n features\n",
    "svm_best = SVC(**grid_search.best_params_)\n",
    "svm_best.fit(X_train_best, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Evaluate performance on testing set\n",
    "score = svm_best.score(X_test_best, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import make_scorer, accuracy_score\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.feature_selection import RFE\n",
    "\n",
    "\n",
    "# # Initialize the feature matrix and target variable\n",
    "\n",
    "# # Initialize the SVM model\n",
    "# svm = SVC(kernel=\"linear\")\n",
    "\n",
    "# # Initialize RFE \n",
    "# rfe = RFE(svm)\n",
    "\n",
    "# # Define the grid of values for the number of features to select and the accuracy required\n",
    "# param_grid = {'n_features_to_select':[60,70,80], \n",
    "#               'estimator__C':[0.1, 1, 10], \n",
    "#               'estimator__kernel':['linear', 'rbf', 'poly'], \n",
    "#               'estimator__gamma': [0.1, 1, 10]}\n",
    "\n",
    "# # Define the scoring function\n",
    "# acc_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "# # Initialize GridSearchCV\n",
    "# grid_search = GridSearchCV(rfe, param_grid, scoring=acc_scorer)\n",
    "\n",
    "# # Fit the grid_search to the data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# best_params = grid_search.best_params_\n",
    "# best_score = grid_search.best_score_\n",
    "# support = grid_search.best_estimator_.support_\n",
    "\n",
    "# # Get the feature names\n",
    "# feature_names = X.columns\n",
    "\n",
    "# # Get the best features\n",
    "# best_features = feature_names[support]\n",
    "\n",
    "# # Print the best parameters, best score, and best features\n",
    "# print(\"Best parameters: \", best_params)\n",
    "# print(\"Best score: \", best_score)\n",
    "# print(\"Best features: \", best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09ee11da7dc62f10eaf9df9c4559184832adcd88d836286cbf9804f9d006cf55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
